{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67174503",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "import gzip\n",
    "import ipaddress\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isdir, isfile, join\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandarallel import pandarallel\n",
    "from joblib import Parallel, delayed\n",
    "from OpenSSL import crypto\n",
    "from local_utils import *\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# so = open(\"shodan.log\", 'w', 10)\n",
    "# sys.stdout.echo = so\n",
    "# sys.stderr.echo = so\n",
    "\n",
    "# get_ipython().log.handlers[0].stream = so\n",
    "# get_ipython().log.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, nb_workers=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# org_ranges_file = 'csv/all_ranges_over_time_gaps_filled.csv'\n",
    "\n",
    "# def process_cols(row):\n",
    "#     row['date'] = pd.to_datetime(row['date'], format='%Y-%m-%d')\n",
    "#     row['first'] = pd.to_datetime(row['first'], format='%Y-%m-%d')\n",
    "#     row['last'] = pd.to_datetime(row['last'], format='%Y-%m-%d')\n",
    "#     row['ripe_names'] = ast.literal_eval(row['ripe_names'])\n",
    "#     row['Ranges'] = [ipaddress.IPv4Network(x) for x in ast.literal_eval(row['Ranges'])]\n",
    "# #     row['ripe_ranges'] = [ipaddress.IPv4Network(x) for x in ast.literal_eval(row['ripe_ranges'])]\n",
    "#     return row\n",
    "    \n",
    "\n",
    "# df = pd.read_csv(org_ranges_file)\n",
    "# df = df.parallel_apply(process_cols, axis=1)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af15de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded = pd.read_csv('csv/all_ranges_exploded.csv', dtype={'ripe_names': 'string'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c25a14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_cols(row):\n",
    "    row['date'] = pd.to_datetime(row['date'], format='%Y-%m-%d')\n",
    "    row['Ranges'] = ipaddress.IPv4Network(row['Ranges'])\n",
    "#     row['ripe_names'] = '' if pd.isna(row['ripe_names']) else row['ripe_names']\n",
    "#     row['ripe_names'] = row['ripe_names'] if row['ripe_names'] != '' else row['org']\n",
    "    return row\n",
    "\n",
    "exploded = exploded.parallel_apply(process_cols, axis=1)\n",
    "exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf8ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded[exploded['ripe_names'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 3 FREAK, last is heartbleed\n",
    "_TLS_VULNS = set(['CVE-2015-0204', 'CVE-2015-1637', 'CVE-2015-1067', 'CVE-2014-0160'])\n",
    "\n",
    "# from risky business paper\n",
    "_WEAK_SIGS = set(['md2', 'md5', 'sha1'])\n",
    "\n",
    "# common primes from https://testssl.sh/etc/common-primes.txt\n",
    "with open('common_primes.txt', 'r') as f:\n",
    "    _DH_COMMON_PRIMES = set([line for line in f.read().split()])\n",
    "\n",
    "# features from Risky Business paper from Edwards et al. and Cloudy with a Chance of Breach\n",
    "def extract_features_from_json_object(json_obj, ip_ranges=None):\n",
    "    if ip_ranges and json_obj['ip_str'] not in ip_ranges:\n",
    "        return {}\n",
    "    \n",
    "    data = {}\n",
    "    data['timestamp'] = json_obj['timestamp'][0:10]\n",
    "    data['port'] = json_obj['port']\n",
    "    data['ip'] = json_obj['ip']\n",
    "    data['ip_str'] = json_obj['ip_str']\n",
    "    \n",
    "    #TLS errors\n",
    "    data['has_tls'] = 0\n",
    "    # silly heuristic to check if it REALLY has TLS services\n",
    "    # (e.g. not a regular http server on port 443 without https)\n",
    "    if 'ssl' in json_obj and 'versions' in json_obj['ssl']:\n",
    "        ssl = json_obj['ssl']\n",
    "        #ssl config errors\n",
    "        data['has_tls'] = 1\n",
    "        \n",
    "        data['tls_old_version'] = 1 if ('SSLv2' in ssl['versions']) or ('SSLv3' in ssl['versions']) else 0\n",
    "            \n",
    "        \n",
    "        try:\n",
    "            data['tls_heartbleed_or_freak_vuln'] = 1 if not _TLS_VULNS.isdisjoint(json_obj['opts']['vulns']) else 0\n",
    "        except:\n",
    "            data['tls_heartbleed_or_freak_vuln'] = 0\n",
    "            \n",
    "        # weak DH keys and primes\n",
    "        dh_weak_bits = False\n",
    "        dh_common_prime = False\n",
    "        if 'dhparams' in ssl and ssl['dhparams']:\n",
    "            dh_bits = ssl['dhparams']['bits']\n",
    "            dh_weak_bits = dh_bits < 2048\n",
    "            dh_prime = ssl['dhparams']['prime']\n",
    "            dh_common_prime = dh_prime in _DH_COMMON_PRIMES\n",
    "            \n",
    "        data['dh_common_prime'] = 1 if dh_common_prime else 0\n",
    "        data['dh_weak_bits'] = 1 if dh_weak_bits else 0\n",
    "            \n",
    "        \n",
    "        #certificate errors\n",
    "        try:\n",
    "            timestamp = datetime.strptime(json_obj['timestamp'], '%Y-%m-%dT%H:%M:%S.%f')\n",
    "        except:\n",
    "            timestamp = datetime.strptime(json_obj['timestamp'], '%Y-%m-%dT%H:%M:%S')\n",
    "            \n",
    "        cert_parse_failed = False\n",
    "        if 'cert' in ssl:\n",
    "            # if parsed certificate data is included in shodan scan data\n",
    "            data['cert_expired'] = 1 if ssl['cert']['expired'] else 0\n",
    "            \n",
    "            try:\n",
    "                issued_time = datetime.strptime(ssl['cert']['issued'], '%Y%m%d%H%M%SZ')\n",
    "            except:\n",
    "                issued_time = datetime.strptime(ssl['cert']['issued'], '%Y%m%d%H%M%S%z').replace(tzinfo=None)\n",
    "                \n",
    "            \n",
    "            weak_key = (ssl['cert']['pubkey']['type'] == 'rsa' and ssl['cert']['pubkey']['bits'] <= 1024) or\\\n",
    "                        ('ec' in ssl['cert']['pubkey']['type'] and ssl['cert']['pubkey']['bits'] < 224)\n",
    "            sig_alg = ssl['cert']['sig_alg'][0:4]\n",
    "            \n",
    "            data['cert_unparseable'] = 0\n",
    "        else:\n",
    "            # old shodan data doesn't parse certificate data, have to do this myself\n",
    "            try:\n",
    "                issued_times = []\n",
    "                expired_times = []\n",
    "                for c in ssl['chain']:\n",
    "                    curr_cert = crypto.load_certificate(crypto.FILETYPE_PEM, c)\n",
    "                    cert_notbefore = str(curr_cert.get_notBefore(), 'utf-8')\n",
    "                    cert_notafter = str(curr_cert.get_notAfter(), 'utf-8')\n",
    "                    try:\n",
    "                        issued_time = datetime.strptime(cert_notbefore, '%Y%m%d%H%M%SZ')\n",
    "                    except:\n",
    "                        issued_time = datetime.strptime(cert_notbefore, '%Y%m%d%H%M%S%z').replace(tzinfo=None)\n",
    "\n",
    "                    try:\n",
    "                        expired_time = datetime.strptime(cert_notafter, '%Y%m%d%H%M%SZ')\n",
    "                    except:\n",
    "                        expired_time = datetime.strptime(cert_notafter, '%Y%m%d%H%M%S%z').replace(tzinfo=None)\n",
    "                        \n",
    "                    issued_times.append(issued_time)\n",
    "                    expired_times.append(expired_time)\n",
    "                \n",
    "                issued_time = max(issued_times)\n",
    "                expired_time = min(expired_times)\n",
    "\n",
    "                cert = crypto.load_certificate(crypto.FILETYPE_PEM, ssl['chain'][0])\n",
    "                data['cert_expired'] = 1 if timestamp > expired_time else 0\n",
    "\n",
    "                pubkey_type = cert.get_pubkey().type()\n",
    "                pubkey_length = cert.get_pubkey().bits()\n",
    "                weak_key = (pubkey_type == crypto.TYPE_RSA and pubkey_length <= 1024) or\\\n",
    "                            (pubkey_type == crypto.TYPE_EC and pubkey_length < 224)\n",
    "                sig_alg = str(cert.get_signature_algorithm(), 'utf-8')[0:4]\n",
    "            \n",
    "                data['cert_unparseable'] = 0\n",
    "            except:\n",
    "                # this means that the certificate could not be parsed\n",
    "                # perhaps invalid, corrupted, no idea\n",
    "                cert_parse_failed = True\n",
    "        \n",
    "        if not cert_parse_failed:\n",
    "            data['cert_issued_in_future'] = 1 if issued_time > timestamp else 0\n",
    "            \n",
    "            chain = ssl['chain']\n",
    "            cert = crypto.load_certificate(crypto.FILETYPE_PEM, chain[0])\n",
    "            root_cert = crypto.load_certificate(crypto.FILETYPE_PEM, chain[-1])\n",
    "            \n",
    "            # non-standard root\n",
    "            # get the certificate's basic constraints extension and check if it indicates that the certificate is a CA\n",
    "            data['cert_nonstandard_root'] = 1\n",
    "            basic_constraints = root_cert.get_extension_count()\n",
    "            for i in range(0, basic_constraints):\n",
    "                ext_name = root_cert.get_extension(i).get_short_name()\n",
    "                if ext_name == b'basicConstraints':\n",
    "                    ext_data = root_cert.get_extension(i).get_data()\n",
    "                    # 0x0 means CA:FALSE, non-zero byte at end of byte string means CA:TRUE\n",
    "                    # extract the last byte from the extension data and check if it is non-zero\n",
    "                    ca_true = ext_data[-1]\n",
    "                    data['cert_nonstandard_root'] = 0 if ca_true else 1\n",
    "                    break\n",
    "            \n",
    "            # broken chain of trust\n",
    "            # make sure the cert is considered not expired for this test\n",
    "            # so get max date from all certs\n",
    "            dates = []\n",
    "            for c in chain:\n",
    "                curr_cert = crypto.load_certificate(crypto.FILETYPE_PEM, c)\n",
    "                cert_notbefore = str(curr_cert.get_notBefore(), 'utf-8')\n",
    "                try:\n",
    "                    d = datetime.strptime(cert_notbefore, '%Y%m%d%H%M%SZ')\n",
    "                except:\n",
    "                    d = datetime.strptime(cert_notbefore, '%Y%m%d%H%M%S%z').replace(tzinfo=None)\n",
    "                dates.append(d)\n",
    "            validation_date = max(dates) + timedelta(days=1)\n",
    "                \n",
    "            store = crypto.X509Store()\n",
    "            store.add_cert(root_cert)\n",
    "            store.set_time(validation_date)\n",
    "            if len(chain) > 2:\n",
    "                intermediate_certs = chain[1:-1]\n",
    "                intermediate_certs = [crypto.load_certificate(crypto.FILETYPE_PEM, x) for x in intermediate_certs]\n",
    "                store_ctx = crypto.X509StoreContext(store, cert, intermediate_certs)\n",
    "            else:\n",
    "                store_ctx = crypto.X509StoreContext(store, cert)\n",
    "            try:\n",
    "                store_ctx.verify_certificate()\n",
    "                data['cert_broken_chain_of_trust'] = 0\n",
    "            except crypto.X509StoreContextError as e:\n",
    "                ex = ast.literal_eval(str(e))\n",
    "                if ex[0] == 2: # cannot find issuer cert\n",
    "                    data['cert_nonstandard_root'] = 1\n",
    "                    data['cert_broken_chain_of_trust'] = 1\n",
    "                elif ex[0] == 10: # one of the certs is expired\n",
    "                    data['cert_expired'] = 1\n",
    "                    data['cert_broken_chain_of_trust'] = 0 # do nothing\n",
    "                elif ex[0] == 18: # self-signed cert\n",
    "                    data['cert_self_signed'] = 1\n",
    "                    data['cert_broken_chain_of_trust'] = 0\n",
    "                elif ex[0] == 20: # unable to get local issuer certificate\n",
    "                    data['cert_broken_chain_of_trust'] = 1\n",
    "                elif ex[0] == 79: # invalid CA certificate\n",
    "                    data['cert_broken_chain_of_trust'] = 1\n",
    "                elif ex[0] == 19: # self-signed cert in chain\n",
    "                    data['cert_broken_chain_of_trust'] = 1\n",
    "                elif ex[0] == 7: # certificate signature failure\n",
    "                    data['cert_broken_chain_of_trust'] = 1\n",
    "                elif ex[0] == 24: # invalid CA cert\n",
    "                    data['cert_broken_chain_of_trust'] = 1\n",
    "                elif ex[0] == 14: # format error in certificate's notAfter field, probably more strict OpenSSL version\n",
    "                    data['cert_broken_chain_of_trust'] = 0 # do nothing\n",
    "                elif ex[0] == 13: # format error in certificate's notBefore field, probably more strict OpenSSL version\n",
    "                    data['cert_broken_chain_of_trust'] = 0 # do nothing\n",
    "                elif ex[0] == 31: # authority and issuer serial number mismatch\n",
    "                    data['cert_broken_chain_of_trust'] = 1\n",
    "                else:\n",
    "                    data['cert_broken_chain_of_trust'] = 1\n",
    "#                    print(e.args)\n",
    "#                    print('is expired?: ' + str(data['cert_expired']))\n",
    "#                    print('nonstandard root?: ' + str(data['cert_nonstandard_root']))\n",
    "#                    print('offending cert start date: ' + str(datetime.strptime(e.certificate.get_notBefore().decode(), '%Y%m%d%H%M%SZ')))\n",
    "#                    print('offending cert expiry date: ' + str(datetime.strptime(e.certificate.get_notAfter().decode(), '%Y%m%d%H%M%SZ')))\n",
    "#                    print('start date: ' + str(datetime.strptime(cert.get_notBefore().decode(), '%Y%m%d%H%M%SZ')))\n",
    "#                    print('expiry date: ' + str(datetime.strptime(cert.get_notAfter().decode(), '%Y%m%d%H%M%SZ')))\n",
    "#                    print('validation date: ' + str(validation_date))\n",
    "                    \n",
    "#                    for i in range(len(chain)):\n",
    "#                        if crypto.load_certificate(crypto.FILETYPE_PEM,chain[i]).get_serial_number() == e.certificate.get_serial_number():\n",
    "#                            print(f'index: {i}')\n",
    "#                    print(chain)\n",
    "                    \n",
    "#                    raise e\n",
    "\n",
    "\n",
    "            # certificates with weak keys\n",
    "            data['cert_weak_key'] = 1 if weak_key else 0\n",
    "\n",
    "            # certificates with weak signatures\n",
    "            data['cert_weak_sig'] = 1 if sig_alg in _WEAK_SIGS else 0\n",
    "\n",
    "            # self-signed certificates\n",
    "            try:\n",
    "                # maybe shodan checked this themselves already\n",
    "                data['cert_self_signed'] = 1 if 'self-signed' in json_obj['tags'] else 0\n",
    "            except:\n",
    "                try:\n",
    "                    # if not, do simple subject == issuer check from parsed data in shodan scan\n",
    "                    data['cert_self_signed'] = 1 if data['ssl']['issuer']['CN'] == data['ssl']['subject']['CN'] else 0\n",
    "                except:\n",
    "                    try:\n",
    "                        # if not parsed by shodan, use our own parsed cert certificate variable\n",
    "                        data['cert_self_signed'] = 1 if cert.get_subject() == cert.get_issuer() else 0\n",
    "                    except:\n",
    "                        # if all else fails, assume it's not self-signed\n",
    "                        data['cert_self_signed'] = 0\n",
    "                    \n",
    "        else:\n",
    "            # if the cert was invalid/corrupted/unparseable, fill these fields with 0\n",
    "            # or maybe add a different field 'cert_corrupted' to cover this issue in the data?\n",
    "            data['tls_old_version'] = 0\n",
    "            data['tls_heartbleed_or_freak_vuln'] = 0\n",
    "            data['dh_common_prime'] = 0\n",
    "            data['dh_weak_bits'] = 0\n",
    "            data['cert_self_signed'] = 0\n",
    "            data['cert_expired'] = 0\n",
    "            data['cert_issued_in_future'] = 0\n",
    "            data['cert_nonstandard_root'] = 0\n",
    "            data['cert_broken_chain_of_trust'] = 0\n",
    "            data['cert_weak_key'] = 0\n",
    "            data['cert_weak_sig'] = 0\n",
    "            data['cert_unparseable'] = 1\n",
    "    \n",
    "    else:\n",
    "        data['tls_old_version'] = 0\n",
    "        data['tls_heartbleed_or_freak_vuln'] = 0\n",
    "        data['dh_common_prime'] = 0\n",
    "        data['dh_weak_bits'] = 0\n",
    "        data['cert_self_signed'] = 0\n",
    "        data['cert_expired'] = 0\n",
    "        data['cert_issued_in_future'] = 0\n",
    "        data['cert_nonstandard_root'] = 0\n",
    "        data['cert_broken_chain_of_trust'] = 0\n",
    "        data['cert_weak_key'] = 0\n",
    "        data['cert_weak_sig'] = 0\n",
    "        data['cert_unparseable'] = 0\n",
    "    \n",
    "    #21 frequently used services\n",
    "    data['ftp'] = 1 if 'ftp' in json_obj else 0\n",
    "    data['telnet'] = 1 if 'telnet' in json_obj else 0\n",
    "    data['sunrpc'] = 1 if json_obj['port'] == 111 else 0  #are we sure about this?\n",
    "    data['netbios'] = 1 if 'netbios' in json_obj else 0\n",
    "    data['snmp'] = 1 if 'snmp' in json_obj else 0\n",
    "    data['smb'] = 1 if 'smb' in json_obj else 0\n",
    "    data['mysql'] = 1 if 'mysql' in json_obj else 0\n",
    "    data['mssql'] = 1 if 'mssql' in json_obj else 0\n",
    "    data['dns'] = 1 if 'dns' in json_obj else 0\n",
    "    data['http'] = 1 if 'http' in json_obj else 0\n",
    "    data['ntp'] = 1 if 'ntp' in json_obj else 0\n",
    "    data['ssh'] = 1 if 'ssh' in json_obj else 0\n",
    "    \n",
    "    try:\n",
    "        module = json_obj['_shodan']['module']\n",
    "        data['smtp'] = 1 if module == 'smtp' else 0\n",
    "        data['pop3'] = 1 if module == 'pop3' else 0\n",
    "        data['imap'] = 1 if module == 'imap' else 0\n",
    "        data['rdp'] = 1 if module == 'rdp' else 0\n",
    "        data['postgres'] = 1 if module == 'postgresql' else 0\n",
    "        data['https'] = 1 if module == 'https' else 0\n",
    "        data['smtps'] = 1 if module == 'smtps' else 0\n",
    "        data['imaps'] = 1 if module == 'imap-ssl' else 0\n",
    "        data['pop3s'] = 1 if module == 'pop3-ssl' else 0\n",
    "    except:\n",
    "        data['smtp'] = 1 if json_obj['port'] == 25 else 0  #other ports?\n",
    "        data['pop3'] = 1 if json_obj['port'] == 110 else 0\n",
    "        data['imap'] = 1 if json_obj['port'] == 143 else 0\n",
    "        data['rdp'] = 1 if json_obj['port'] == 3389 else 0\n",
    "        data['postgres'] = 1 if json_obj['port'] == 5432 else 0\n",
    "        data['https'] = 1 if json_obj['port'] == 443 else 0\n",
    "        data['smtps'] = 1 if json_obj['port'] == 465 or json_obj['port'] == 587 else 0\n",
    "        data['imaps'] = 1 if json_obj['port'] == 993 else 0\n",
    "        data['pop3s'] = 1 if json_obj['port'] == 995 else 0\n",
    "        \n",
    "    # cloudy with a chance of breach features\n",
    "    if 'dns' in json_obj:\n",
    "        dns = json_obj['dns']\n",
    "        try:\n",
    "            data['dns_recursive'] = 1 if dns['recursive'] else 0\n",
    "        except:\n",
    "            data['dns_recursive'] = 0\n",
    "    else:\n",
    "        data['dns_recursive'] = 0\n",
    "        \n",
    "        \n",
    "    # other stuff, necessary?\n",
    "    data['ntlm'] = 1 if 'ntlm' in json_obj else 0\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_shodan_file(ip_ranges, file_path):\n",
    "    data_list = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            parsed = json.loads(line.strip())\n",
    "\n",
    "            data = extract_features_from_json_object(json_obj=parsed, ip_ranges=ip_ranges)\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "\n",
    "            vulns = []\n",
    "            if 'vulns' in parsed:\n",
    "                for k in parsed['vulns'].keys():\n",
    "                    if 'CVE' in k:\n",
    "                        vulns.append(k)\n",
    "            data['vulns'] = vulns\n",
    "\n",
    "            data_list.append(data)\n",
    "\n",
    "    return data_list\n",
    "\n",
    "def process_shodan_files(ripe_name, ip_ranges, paths_list):\n",
    "    processed_path = '/data/data-sh/data/data.external-scans/shodan/ripe-nl/processed'\n",
    "    done_file_path = f'{processed_path}/_done'\n",
    "        \n",
    "    data_list = Parallel(n_jobs=40, verbose=1, backend='multiprocessing')(delayed(process_shodan_file)(ip_ranges, file_path) for file_path in paths_list)\n",
    "    data_list = [ item for sublist in data_list for item in sublist ]\n",
    "\n",
    "    data_df = pd.DataFrame(data_list).fillna(int(0))\n",
    "    if len(data_df.index) > 0:\n",
    "        print('\\tsaving files...')\n",
    "        for i, (name, group) in enumerate(data_df.groupby('timestamp')):\n",
    "            group.to_csv(f'{processed_path}/{name}-{ripe_name}.csv', index=False)\n",
    "    else:\n",
    "        print('\\tempty dataframe, skipping')\n",
    "    \n",
    "    with open(done_file_path, 'a') as f:\n",
    "        f.write(f'{ripe_name}\\n')\n",
    "\n",
    "def process_shodan_files_non_parallel(ripe_name, ip_ranges, paths_list):\n",
    "    processed_path = '/data/data-sh/data/data.external-scans/shodan/ripe-nl/processed'\n",
    "    done_file_path = f'{processed_path}/_done'\n",
    "        \n",
    "    data_list = []\n",
    "    size = len(paths_list)\n",
    "    start = datetime.now()\n",
    "    print(f'total files to process: {size}')\n",
    "    for i in range(len(paths_list)):\n",
    "        file_path = paths_list[i]\n",
    "        data_list.append( process_shodan_file(ip_ranges, file_path) )\n",
    "        \n",
    "        if i < 500 and i%50==0:\n",
    "            seconds = (datetime.now()-start).seconds\n",
    "            print(f'Done with {i+1} out of {size}\\t in {seconds} seconds')\n",
    "            \n",
    "        elif i % 500 == 0:\n",
    "            seconds = (datetime.now()-start).seconds\n",
    "            print(f'Done with {i+1} out of {size}\\t in {seconds} seconds')\n",
    "           \n",
    "    seconds = (datetime.now()-start).seconds \n",
    "    print(f'Done\\t in {seconds} seconds')\n",
    "    \n",
    "    data_list = [ item for sublist in data_list for item in sublist ]\n",
    "\n",
    "    data_df = pd.DataFrame(data_list).fillna(int(0))\n",
    "    if len(data_df.index) > 0:\n",
    "        print('\\tsaving files...')\n",
    "        for i, (name, group) in enumerate(data_df.groupby('timestamp')):\n",
    "            group.to_csv(f'{processed_path}/{name}-{ripe_name}.csv', index=False)\n",
    "    else:\n",
    "        print('\\tempty dataframe, skipping')\n",
    "    \n",
    "    with open(done_file_path, 'a') as f:\n",
    "        f.write(f'{ripe_name}\\n')\n",
    "\n",
    "def get_set_of_ips(ip_ranges, ip_type='str', parallel=False):\n",
    "    if ip_type == 'str':\n",
    "        iparray = [ host.compressed for ip_range in ip_ranges for host in ip_range ]\n",
    "    elif ip_type == 'int':\n",
    "        if parallel:\n",
    "            iparray = Parallel(n_jobs=40, verbose=1, backend='multiprocessing')(\n",
    "                delayed(get_hosts_in_range)(ip_range)\n",
    "                for ip_range in ip_ranges\n",
    "            )\n",
    "            iparray = [ item for sublist in iparray for item in sublist ]\n",
    "        else:\n",
    "            iparray = [ int(host) for ip_range in ip_ranges for host in ip_range ]\n",
    "    else:\n",
    "        iparray = [ host for ip_range in ip_ranges for host in ip_range ]\n",
    "    ipset = set(iparray)\n",
    "    return ipset\n",
    "\n",
    "\n",
    "def get_hosts_in_range(r):\n",
    "    res = []\n",
    "    for host in r:\n",
    "        res.append(int(host))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f670de7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### SHODAN JSON FILES ON SERVER (1234567890-2143658709.json)\n",
    "###### ARE NOT IN ORDER, SO ALL FILES NEED TO BE PROCESSED\n",
    "###### :'(\n",
    "\n",
    "shodan_files_path = '/data/data-ig/personal/carlos/cidr_NL/results_shodan'\n",
    "shodan_dirs_to_process = sorted([ x for x in exploded['ripe_names'].unique() if x.startswith('nl.') ])\n",
    "\n",
    "processed_path = '/data/data-sh/data/data.external-scans/shodan/ripe-nl/processed'\n",
    "done_file_path = f'{processed_path}/_done'\n",
    "if not os.path.isfile(done_file_path):\n",
    "    with open(done_file_path, 'w') as f:\n",
    "        pass\n",
    "\n",
    "with open(done_file_path, 'r') as f:\n",
    "    done_files = [ l.strip() for l in f.readlines() ]\n",
    "\n",
    "for ripe_name in shodan_dirs_to_process:\n",
    "    if ripe_name == 'nl.kpn-bbt' or ripe_name == 'nl.wapi':\n",
    "        continue\n",
    "\n",
    "    print(f'processing {ripe_name}')\n",
    "    if ripe_name in done_files:\n",
    "        print('\\talready done, skipping.')\n",
    "        continue\n",
    "    \n",
    "    ip_ranges = exploded.loc[exploded['ripe_names'] == ripe_name, 'Ranges'].unique()\n",
    "    ip_set = get_set_of_ips(ip_ranges)\n",
    "    current_path = f'{shodan_files_path}/{ripe_name}'\n",
    "    shodan_files = [f'{current_path}/{f}' for f in listdir(current_path) if isfile(join(current_path, f))]\n",
    "    process_shodan_files_non_parallel(ripe_name, ip_set, shodan_files)\n",
    "\n",
    "for ripe_name in ['nl.kpn-bbt', 'nl.wapi']:\n",
    "    print(f'processing {ripe_name}')\n",
    "    if ripe_name in done_files:\n",
    "        print('\\talready done, skipping.')\n",
    "        continue\n",
    "        \n",
    "    ip_ranges = exploded.loc[exploded['ripe_names'] == ripe_name, 'Ranges'].unique()\n",
    "    ip_set = get_set_of_ips(ip_ranges)\n",
    "    current_path = f'{shodan_files_path}/{ripe_name}'\n",
    "    shodan_files = [f'{current_path}/{f}' for f in listdir(current_path) if isfile(join(current_path, f))]\n",
    "    process_shodan_files_non_parallel(ripe_name, ip_set, shodan_files)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefa771c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_nonripe_shodan_file(file_path, ip_ranges):\n",
    "    dirname = os.path.dirname(file_path)\n",
    "    processed_path = f'{dirname}/processed/'\n",
    "    filename = os.path.basename(file_path).replace('.json', '')\n",
    "    \n",
    "    data_list = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            parsed = json.loads(line.strip())\n",
    "\n",
    "            data = extract_features_from_json_object(json_obj=parsed, ip_ranges=ip_ranges)\n",
    "            if len(data) == 0:\n",
    "                continue\n",
    "\n",
    "            vulns = []\n",
    "            if 'vulns' in parsed:\n",
    "                for k in parsed['vulns'].keys():\n",
    "                    if 'CVE' in k:\n",
    "                        vulns.append(k)\n",
    "            data['vulns'] = vulns\n",
    "\n",
    "            data_list.append(data)\n",
    "            \n",
    "    return data_list\n",
    "    \n",
    "    \n",
    "nonripe_shodan_dir = '/data/data-sh/data/data.external-scans/shodan/non-ripe-nl/'\n",
    "nonripe_shodan_files = [f for f in listdir(nonripe_shodan_dir) if isfile(join(nonripe_shodan_dir, f)) and f.endswith('.csv.gz')]\n",
    "nonripe_shodan_files = sorted(nonripe_shodan_files, key=lambda x: int(x.split('-')[0]))\n",
    "\n",
    "nonripe = exploded[~exploded['ripe_names'].str.startswith('nl.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46747744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "processed_path = f'{nonripe_shodan_dir}/processed'\n",
    "\n",
    "done_file_path = f'{processed_path}/_done'\n",
    "if not os.path.isfile(done_file_path):\n",
    "    with open(done_file_path, 'w') as f:\n",
    "        pass\n",
    "        \n",
    "with open(done_file_path, 'r') as f:\n",
    "    done_files = [ l.strip() for l in f.readlines() ]\n",
    "\n",
    "for i, (name, group) in enumerate(nonripe.groupby('ripe_names')):\n",
    "    print(f'processing {name}')\n",
    "    \n",
    "    if name in done_files:\n",
    "#         print('\\talready done, skipping.')\n",
    "        continue\n",
    "        \n",
    "    print('\\tpicking which shodan files to process')\n",
    "    ip_ranges = group['Ranges'].unique()\n",
    "    ip_set = get_set_of_ips(ip_ranges, ip_type='int', parallel=True)\n",
    "    sorted_ip_list = sorted(list(ip_set))\n",
    "    \n",
    "    # IMPORTANT THAT nonripe_shodan_files IS SORTED NUMERICALLY\n",
    "    nonripe_shodan_files_to_process = []\n",
    "    for iprange in ip_ranges:\n",
    "        first_ip = int(iprange[0])\n",
    "        last_ip = int(iprange[-1])\n",
    "        index = 0\n",
    "        done_range = False\n",
    "        # for every shodan file\n",
    "        for _ in range(len(nonripe_shodan_files)):\n",
    "            f = nonripe_shodan_files[index]\n",
    "            shodan_filename = f.replace('.csv.gz', '')\n",
    "            ips = shodan_filename.split('-')\n",
    "            ip1 = int(ips[0])\n",
    "            ip2 = int(ips[1])\n",
    "            # if the first IP of this range is in the current file, start adding files\n",
    "            if first_ip <= ip2:\n",
    "                nonripe_shodan_files_to_process.append(f)\n",
    "                index += 1\n",
    "                # while the last IP in this range is larger than the first IP in the current shodan file,\n",
    "                # add the current shodan file to list of files to process\n",
    "                while True:\n",
    "                    if index >= len(nonripe_shodan_files):\n",
    "                        done_range = True\n",
    "                        break\n",
    "                    f2 = nonripe_shodan_files[index]\n",
    "                    shodan_filename2 = f2.replace('.csv.gz', '')\n",
    "                    ips2 = shodan_filename2.split('-')\n",
    "                    ip21 = int(ips2[0])\n",
    "                    ip22 = int(ips2[1])\n",
    "                    if last_ip >= ip21:\n",
    "                        nonripe_shodan_files_to_process.append(f2)\n",
    "                        index += 1\n",
    "                        \n",
    "                    else:\n",
    "                        # once the last IP in this range is smaller than the last IP in the current shodan file,\n",
    "                        # stop adding files\n",
    "                        # and start processing next range\n",
    "                        print(f'breaking because {last_ip} less than {ip21}')\n",
    "                        \n",
    "                        done_range = True\n",
    "                        break\n",
    "                        \n",
    "            if done_range:\n",
    "                break\n",
    "            \n",
    "            index += 1\n",
    "            \n",
    "    nonripe_shodan_files_to_process = list(set(nonripe_shodan_files_to_process))\n",
    "                    \n",
    "        \n",
    "    print(nonripe_shodan_files_to_process)\n",
    "    print('\\tprocessing files')\n",
    "    ip_set = get_set_of_ips(ip_ranges)\n",
    "    \n",
    "    \n",
    "    #for every file in nonripe_shodan_files_to_process\n",
    "    #    extract all rows where row IP is in ip_set\n",
    "    data_df = []\n",
    "    for filename in nonripe_shodan_files_to_process:\n",
    "        try:\n",
    "            tmp = pd.read_csv(f'{nonripe_shodan_dir}/{filename}', compression='gzip')\n",
    "        except:\n",
    "            print('error in %s' % filename)\n",
    "            raise\n",
    "        data_df.append(tmp)\n",
    "    if len(data_df) > 0:\n",
    "        data_df = pd.concat(data_df)\n",
    "        if len(data_df.index) > 0:\n",
    "            print('\\tsaving files...')\n",
    "            for i, (date, group) in enumerate(data_df.groupby('timestamp')):\n",
    "                group.to_csv(f'{processed_path}/{date}-{name}.csv', index=False)\n",
    "        else:\n",
    "            print('\\tempty dataframe, skipping')\n",
    "            \n",
    "    else:\n",
    "        print('\\tempty shodan files, skipping')\n",
    "        \n",
    "    \n",
    "    with open(done_file_path, 'a') as f:\n",
    "        f.write(f'{name}\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d85b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done processing non-shodan files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80da54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_shodan_df(row):\n",
    "    row['ip'] = ipaddress.IPv4Network(row['ip_str'])\n",
    "    return row\n",
    "\n",
    "_FEATURES = ['port', 'ip', 'has_tls', 'tls_old_version', 'tls_heartbleed_or_freak_vuln', 'dh_common_prime', 'dh_weak_bits', 'cert_expired', 'cert_issued_in_future', 'cert_weak_key', 'cert_weak_sig', 'cert_nonstandard_root', 'cert_broken_chain_of_trust', 'cert_self_signed', 'ftp', 'telnet', 'sunrpc', 'netbios', 'snmp', 'smb', 'mysql', 'mssql', 'dns', 'http', 'ntp', 'ssh', 'smtp', 'pop3', 'imap', 'rdp', 'postgres', 'https', 'smtps', 'imaps', 'pop3s', 'vulns']\n",
    "_EMPTY_FEATURES = {f: 0 for f in _FEATURES}\n",
    "def extract_features_group(group, super_dict):\n",
    "    def get_closest_date(current_date):\n",
    "        dates = sorted(list(date_files_dict.keys()))\n",
    "        for d in dates:\n",
    "            if d > current_date:\n",
    "                return d\n",
    "            \n",
    "        return None\n",
    "    \n",
    "    result = pd.Series([ {} for x in group.index ], index=group.index, name='shodan_features')\n",
    "    \n",
    "    date = group['date'].iloc[0]\n",
    "    ripe_name = group['ripe_names'].iloc[0]\n",
    "    \n",
    "    if ripe_name not in super_dict:\n",
    "        result = pd.Series([ _EMPTY_FEATURES for x in group.index ], index=group.index, name='shodan_features')\n",
    "        return result\n",
    "\n",
    "    date_files_dict = super_dict[ripe_name]\n",
    "\n",
    "    sorted_dates = sorted(list(date_files_dict.keys()))\n",
    "    first_date = sorted_dates[0]\n",
    "    last_date = sorted_dates[-1]\n",
    "    if date < first_date:\n",
    "        return result\n",
    "    if date > last_date:\n",
    "        return result\n",
    "\n",
    "    closest = date_files_dict.get(date, 0)\n",
    "    if closest == 0:\n",
    "        closest_date = get_closest_date(date)\n",
    "        closest = date_files_dict.get(closest_date)\n",
    "\n",
    "    if os.path.getsize(closest) == 0:\n",
    "        return result\n",
    "\n",
    "    shodan_df = pd.read_csv(closest)\n",
    "    shodan_df = shodan_df.apply(parse_shodan_df, axis=1)\n",
    "    shodan_df['vulns'] = shodan_df['vulns'].apply(ast.literal_eval)\n",
    "    result = group.apply(extract_features, args=(shodan_df,), axis=1)\n",
    "    return result['shodan_features']\n",
    "    \n",
    "def extract_features(row, shodan_df):\n",
    "    #get single IP range from Ranges column\n",
    "    r = row['Ranges']\n",
    "    #extract features from shodan dataframe\n",
    "    issupernet = r.supernet_of\n",
    "    subnet_ips = shodan_df[ [ issupernet(ip) for ip in shodan_df['ip'] ] ]\n",
    "    \n",
    "    cols = list(shodan_df.columns.values)\n",
    "    cols.remove('timestamp')\n",
    "    cols.remove('ip_str')\n",
    "    operations = { col: 'sum' for col in cols }\n",
    "    operations['ip'] = 'nunique'\n",
    "    operations['port'] = 'count'\n",
    "    operations['vulns'] = 'sum'\n",
    "    subnet_ips['vulns'] = subnet_ips['vulns'].apply(len)\n",
    "    features_df = subnet_ips.agg(operations)\n",
    "    features_dict = features_df.to_dict()\n",
    "    \n",
    "    row['shodan_features'] = features_dict\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824f348",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "shodan_output_dir = '/data/data-sh/data/data.external-scans/shodan'\n",
    "ripe_shodan_output_dir = '/data/data-sh/data/data.external-scans/shodan/ripe-nl'\n",
    "ripe_shodan_processed_path = f'{ripe_shodan_output_dir}/processed'\n",
    "nonripe_shodan_output_dir = '/data/data-sh/data/data.external-scans/shodan/non-ripe-nl'\n",
    "nonripe_shodan_processed_path = f'{nonripe_shodan_output_dir}/processed'\n",
    "\n",
    "if not os.path.isfile(f'{shodan_output_dir}/exploded.csv'):\n",
    "\n",
    "    cols = list(exploded.columns.values)\n",
    "    cols.remove('Ranges')\n",
    "    cols.remove('ripe_names')\n",
    "\n",
    "    # HOW TO PARALLEL_APPLY FOR BATCH OF ROWS, WITHOUT COMBINING INTO GROUPS\n",
    "    # https://stackoverflow.com/questions/72845864/call-parallel-apply-for-batch-of-rows\n",
    "\n",
    "    ripe_names_list = sorted(exploded['ripe_names'].unique().tolist())\n",
    "\n",
    "    # collect all processed shodan files, group them by RIPE name + date\n",
    "    super_dict = {}\n",
    "    for name in ripe_names_list:\n",
    "        \n",
    "        if name.startswith('nl.'):\n",
    "            ripe_or_nonripe_path = ripe_shodan_processed_path\n",
    "        else:\n",
    "            ripe_or_nonripe_path = nonripe_shodan_processed_path\n",
    "            \n",
    "        files = glob.glob(f'{ripe_or_nonripe_path}/*{name}*')\n",
    "        if len(files) == 0:\n",
    "            continue\n",
    "\n",
    "        last_slash_index = files[0].rfind('/')\n",
    "\n",
    "        dates = [ x[last_slash_index+1 : last_slash_index + 1 + len('2010-10-10')] for x in files ]\n",
    "        dates = [ datetime.strptime(x, '%Y-%m-%d') for x in dates ]\n",
    "        date_files_dict = dict( zip(dates, files) )\n",
    "        super_dict[name] = date_files_dict\n",
    "    \n",
    "    \n",
    "    pandarallel.initialize(progress_bar=True, nb_workers=5)\n",
    "    exploded['shodan_features'] = (exploded.groupby(['date', 'ripe_names']).parallel_apply(lambda x: extract_features_group(x, super_dict)).droplevel(0).droplevel(0))\n",
    "\n",
    "    exploded.to_csv(f'{shodan_output_dir}/exploded.csv', index=False)\n",
    "\n",
    "else:\n",
    "    exploded = pd.read_csv(f'{shodan_output_dir}/exploded.csv')\n",
    "#     exploded['date'] = pd.to_datetime(exploded['date'], format='%Y-%m-%d')\n",
    "#     exploded['first'] = pd.to_datetime(exploded['first'], format='%Y-%m-%d')\n",
    "#     exploded['last'] = pd.to_datetime(exploded['last'], format='%Y-%m-%d')\n",
    "    exploded['Ranges'] = exploded['Ranges'].apply(ipaddress.IPv4Network)\n",
    "#     exploded['ripe_ranges'] = exploded['ripe_ranges'].apply(ipaddress.IPv4Network)\n",
    "    exploded['shodan_features'] = exploded['shodan_features'].apply(ast.literal_eval)\n",
    "    print('loaded exploded.csv from disk')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a924e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded = exploded.sort_values(['date', 'customer', 'Ranges'])\n",
    "exploded[exploded['shodan_features'].str.len() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a046f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "exploded = exploded.sort_values(['date', 'customer', 'Ranges'])\n",
    "exploded['Ranges'] = exploded['Ranges'].apply(str)\n",
    "\n",
    "def aggregate_to_list(group):\n",
    "    return group.tolist()\n",
    "\n",
    "imploded = exploded.groupby(cols).agg({'Ranges': aggregate_to_list,\n",
    "                                       'ripe_names': aggregate_to_list,\n",
    "                                       'shodan_features': aggregate_to_list}).reset_index()\n",
    "\n",
    "imploded.to_csv(f'{shodan_output_dir}/imploded.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8385b",
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "###### i think i'll probably need to use the exploded versions when processing further and ML stuff\n",
    "################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
